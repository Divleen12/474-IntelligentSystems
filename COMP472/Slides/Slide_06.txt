AI


Artificial Intelligence: 
Deep Learning, CNNs
many slides from:  Y. Bengio, A. Ng and Y. LeCun

1



Today
1. Motivation
2. Feature Learning
3. Early Training of Deep Neural Networks
4. CNNs for Image Processing
5. Conclusion

2



History of AI

3



Deep Learning in the Academic Press (2012-2015)

4



Deep Learning in the News (2013)

Slide from Yoshua Bengio, 2015 5



Deep Learning in the News (2012-2014)

Slide from Yoshua Bengio, 2015 6



Major Breakthroughs 
 Speech Recognition & Machine Translation (2010+)

 Skype Translator

 Image Recognition & Computer Vision (2012+)
 Natural Language Processing (2014+) 
 …

Figure from Yoshua Bengio, 2015

Google Translate

Skype Translator

Google now

7



Major Breakthroughs 
 Speech Recognition & Machine Translation (2010+) 
 Image Recognition & Computer Vision (2012+)

 Natural Language Processing (2014+)
 …

Figure from Yoshua Bengio, 2015

3.567% 3.581%

4th year

Object recognition Self driving cars

8



Major Breakthroughs 

Image Captioning (deep vision + deep NLP)

Question Answering

 Speech Recognition & Machine Translation (2010+)
 Image Recognition & Computer Vision (2012+)

 Natural Language Processing (2014+)

9



A

B

Image Captioning: Better than humans?

10



Today
1. Motivation
2. Feature Learning
3. Early Training of Deep Neural Networks
4. CNNs for Image Processing
5. Deep Learning for NLP
6. Conclusion

11



A Deep Neural Net

https://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-c
https://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/

The Google “Inception” deep neural network architecture for image recognition 
(27 layers) 

12

https://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-c
https://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-c


Matrix Notation

13

We now represent inputs x, weights W and the bias weights b as matrices:

Input to hidden: 
1. Compute net activation neth = x · Wih + bih

2.Compute activation function (e.g., sigmod): h = S(neth)
Hidden to output: 

o = S(h · Who + bho)

 → Worksheet #5 (“Matrix Notation”)
https://towardsdatascience.com/neural-networks-i-notation-and-building-blocks-817b1d2ea04b



Initial Drawbacks
1. Standard backpropagation with sigmoid activation function 

does not scale well with multiple layers
 Weight of early layers change too slowly (no learning)

2. Overfitting
 Large network -> lots of parameters -> increased capacity to 

“learn by heart”
3. Multilayered ANNs need lots of labeled data 

 Most data is not labeled :(

14



Initial Drawbacks (1)
1. Standard gradient-based backpropagation does not scale well with 

multiple layers…

When we multiply the gradients many times (for 
each layer),  it can lead to …

a) Vanishing gradient problem: 
 gradients shrink exponentially with the 

number of layers 
 so weight updates get smaller and smaller
 and weights of early layers change very 

slowly and network learns very very slowly
b) Exploding gradient problem:

 multiplying gradients could also make them 
grow exponentially.  

 so weight updates get larger and larger
 and the weights can become so large as to 

overflow and result in NaN values

15



Initial Drawbacks (1)

https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f

https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6

To help, we can :
a) Use other activation 

functions…

b) Do “gradient 
clipping” (i.e. set bounds 
on the gradients)

16

https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f
https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f
https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6
https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6


Initial Drawbacks (2)

https://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html

 

2. Overfitting
 Large network -> lots of parameters -> 

increased capacity to “learn by heart”
 Solutions:

 Regularization: 
 modify the error function that we minimize to penalize large weights.

 where f(w) grows larger as the weights grow larger and λ is the 
regularization strength

 Dropout: 
 keep a neuron active with some probability p or setting it to 

zero otherwise. 
 prevents the network from becoming too dependent on any 

one neuron. 

17



Initial Drawbacks (3)
3. Multilayered ANNs need lots of labeled data 

 Solution: “pre-train” the network with features found 
automatically using unsupervised data 

 i.e. Automatic feature learning… (see next few slides)

18



Classic ML

Input

Motorbikes
“Non”-Motorbikes

Learning
algorithm

Feature 1

Feature 2

Manual Extraction of Features 
(eg. edge detection, colors, 

texture,…)

Classic ML, 
requires labeled 
data and hand-
crafted features

1. Needs expert 
knowledge

2. Time-consuming 
     and expensive

3. Does not     
    generalize to 
    other domains

Slide from Y. LeCun
19



Automatic Feature Learning

Input

Motorbikes
“Non”-Motorbikes

Automatic 
Feature 

Representation
Learning
algorithm

“wheel”

“handle”

handle

wheel
eg. handle, wheel, … With Automatic 

Feature Learning:

1. We feed the 
network the raw data 
(not feature-
curated)

2. The features are 
learned by the 
network

3. Features learned 
can be re-used in 
similar tasks. 

20
Slide from Y. LeCun



Automatic Feature Learning

https://www.strong.io/blog-images/movie-posters/Slide6.png 21



Automatic Feature Learning

 Each layer learns more abstract features that are then combined / 
composed into higher-level features automatically

 Like the human brain … 
 has many layers of neurons which act as feature detectors
 detecting more and more abstract features as you go up

 E.g. to classify an image of a cat:
 Bottom Layers: Edge detectors, curves, corners straight lines
 Middle Layers: Fur patterns, eyes, ears
 Higher Layers: Body, head, legs
 Top Layer: Cat or Dog

Deep Learning = Machine learning algorithms based on 
learning multiple levels of representation / abstraction. 
– Y. Bengio

22



Automatic Feature Learning

23



What Types of Features?
 For image recognition 

 pixel -> edge -> texton -> motif -> part -> object

 For NLP
 character -> word ->  constituents -> clause -> sentence -> discourse

 For speech:
 sample  spectral band -> sound -> … phone -> phoneme -> word→

Figure from Y LeCun 24



Eg: Learning Image Features
Faces Cars Elephants Chairs

Actual images 
(pixels)

25

Learned 
object 
parts

Learned 
edges

Examples of learned objects parts from object categories

Learned 
objects

Learned features / 
representations can be used in  
variety of OTHER classification 

tasks…  deep learning→



Advantages of Unsupervised Feature 
Learning 

26



Advantages of Unsupervised 
Feature Learning 
 Much more unlabeled data available than labeled data:

 Eg. Websites, Books, Videos, Pictures
 Humans learn initially from unlabeled examples

 Eg. Babies learn to talk/recognize objects without labeled data
 As the features are learned in an unsupervised way from a 

different and larger dataset, less risk of over-fitting
 No need for manual feature engineering 
 These features are organized into multiple levels

 Each level creates new features from combinations of features from 
the level below

 Each level is more abstract than the ones below (hierarchy of 
features)

27



Today
1. Motivation
2. Feature Learning
3. Early Training of Deep Neural Networks
4. CNNs for Image Processing
5. Deep Learning for NLP
6. Conclusion

28



General Architecture of a Deep Network

1. Unsupervised pre-training of neural network using 
unlabeled data
 aka. unsupervised learning of features

2. Supervised training with labeled data using features 
learned from above with a standard classifier
 Eg. an ANN, SVM, … 

29



Training a Deep NN

then this 
layer

finally 
this layer

then this 
layer

then this 
layer

train this 
layer first

EACH of the (non-output) layers is trained to 
learn a representation of the data (eg. 
autoencoder) aka “pretraining the network 
with unsupervised data”

Use pretrained 
representations to feed a 
regular ANN with a smaller 
set of labelled data. 

30



Autoencoders
 To learn a representation of the data, we can use:

1. Deep Belief Networks 
 Mid 2000’s: Geoffrey Hinton trains a deep network by: 

 Stacking  Restricted Boltzmann Machines (RBM’s) on 
top of one another – deep belief network

 Training layer by layer on un-labeled data
 Then, using back propagation to fine tune weights on 

labeled data

2. Autoencoders
 2006: Yoshua Bengio et al. does something similar using auto-

encoders instead of RBM’s

 Both are 2 layer neural networks that learn to model 
their inputs

31



Autoencoder
 The network is trained 

to output the input
 i.e. learn the identity 

function. 

 Trivial… unless, we 
impose constraints:

 Nb of units in layer 2 < nb 
of input units (learn 
compressed 
representation) 

OR
 Constrain layer 2 to be 

sparse (i.e. many 
connections are 
“disabled”)  

x4

x5

x6

+1

layer 1 
(input)

layer 2

x1

x2

x3

x4

x5

x6

x1

x2

x3

+1

layer 3 
(output)

a1

a2

a3

32 → Worksheet #5 (“Autoencoder”)



Autoencoder

x4

x5

x6

+1

input 
layer

layer 2

x1

x2

x3

x4

x5

x6

x1

x2

x3

+1

a1

a2

a3

Feedforward input

Backprop error

output 
layer

33

 → Worksheet #5 
(“Autoencoder Activation”)



Autoencoder

x4

x5

x6

+1

input 
layer

layer 2

x1

x2

x3

x4

x5

x6

x1

x2

x3

+1

a1

a2

a3

output 
layer

34



Autoencoder

New (compressed) representation of 
the input to be fed to the next layer

i.e. the encoded x 

x4

x5

x6

+1

layer 2

x1

x2

x3
a1

a2

a3

 input layer

 

35



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

Train parameters (weights)
But keep bi’s sparse (ie. many 
zeros). 

Autoencoder

new input 
layer

new output
 layer

Feedforward input

Backprop error

a1

a2

a3

36



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

Autoencoder

new input 
layer

new output
 layer

a1

a2

a3

37



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

Autoencoder

New representation for the 
input to the next layer

i.e. the encoded a

 

38



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

+1

c1

c2

c3

Autoencoder

new input 
layer

new output 
layer

Train parameters
subject to ci’s being sparse. 

Feedforward input

Backprop error

b1

b3

b2

39



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

+1

c1

c2

c3

Autoencoder

new input 
layer

new output 
layer

b1

b2

b3

40



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

+1

c1

c2

c3

New representation 
for input. 

Use [c1, c3, c3] as representation to 
feed to supervised learning algorithm 

(the last, supervised, layer).

Autoencoder

 

41



Training a Deep NN

then this 
layer

finally 
this layer

then this 
layer

then this 
layer

train this 
layer first

Use of unlabelled data to “pretrain” the network
i.e. learn more and more abstract feature 
representations

Use pretrained 
representations to feed 
a regular ANN with a 
smaller set of labelled 
data. 

42



Many Types of Neural Networks 
 

https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463 43



Many Types of Deep Networks (con’t) 

https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463 44



Today
1. Motivation
2. Feature Learning
3. Early Training of Deep Neural Networks
4. CNNs for Image Processing
5. Conclusion

45



CNNs for Image Processing

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

Image of a 4 in grey scale
Value = 0-> white …. 255->black

CNNs = Convolutional Neural Networks

46



CNNs for Image Processing

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

 Standard input of the image in the ANN:
1. 1 pixel = 1 input. 
2. Eg. color image of 200x200x 3channels (RGB) 
       --> in a fully connected ANN, a neuron of the     
            input layer has 200*200*3 = 120,000 weights
       --> huge number of parameters, can easily
             overfit
2. We linearize the image ==> We lose spatial 

information

…

47



Convolutional Layer
 Use a filter (aka kernel) that “convolves” on the image
 Filter = small weight matrix to learn

 
 

1 0.3
0.5 2

219.2 23.9 147.2
283.9 22.9 349.5

92.6 16.1 195.4
139.6 20.4 377.7
121.9 9.9 417.5

223.8 13.6 341.4
620.4 268.2 443.6
486.1 731.2 736

86 4 8 184
252 3 8 40

34 7 7 163
105 2 3 69
56 3 8 175

126 1 2 178
163 8 4 142
22 222 74 180

163 158 204 253

48



Convolutional Layer
 Use a filter (aka kernel) that “convolves” on the image
 Filter = small weight matrix to learn

 
 

1 0.3
0.5 2

219.2 23.9 147.2
283.9 22.9 349.5

92.6 16.1 195.4
139.6 20.4 377.7
121.9 9.9 417.5

223.8 13.6 341.4
620.4 268.2 443.6
486.1 731.2 736

86 4 8 184
252 3 8 40

34 7 7 163
105 2 3 69
56 3 8 175

126 1 2 178
163 8 4 142
22 222 74 180

163 158 204 253

49



Convolutional Layer
 Use a filter (aka kernel) that “convolves” on the image
 Filter = small weight matrix to learn

 
 

1 0.3
0.5 2

219.2 23.9 147.2
283.9 22.9 349.5

92.6 16.1 195.4
139.6 20.4 377.7
121.9 9.9 417.5

223.8 13.6 341.4
620.4 268.2 443.6
486.1 731.2 736

86 4 8 184
252 3 8 40

34 7 7 163
105 2 3 69
56 3 8 175

126 1 2 178
163 8 4 142
22 222 74 180

163 158 204 253

50



Convolutional Layer
 Use a filter (aka kernel) that “convolves” on the image
 Filter = small weight matrix to learn

 
 

1 0.3
0.5 2

219.2 23.9 147.2
283.9 22.9 349.5

92.6 16.1 195.4
139.6 20.4 377.7
121.9 9.9 417.5

223.8 13.6 341.4
620.4 268.2 443.6
486.1 731.2 736

86 4 8 184
252 3 8 40

34 7 7 163
105 2 3 69
56 3 8 175

126 1 2 178
163 8 4 142
22 222 74 180

163 158 204 253

51



Learn the Filters

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

 The weight matrix (filter/kernel) behaves like a filter 
 The network learns the values of the filter(s) that activate when they 

“see” some visual feature that is useful to identify the object (the final 
classification)
 Ex. a horizontal line, a blotch of some color, a circle… 

 

429 505 686 856
261 792 412 640
633 653 851 751
608 913 713 657

1 0 1
0 1 0
1 0 1

18 54 51 239 244 188
55 121 75 78 95 88
35 24 204 113 109 221

3 154 104 235 25 130
15 253 225 159 78 233
68 85 180 214 245 0

52



Convolution Hyper-parameters

1. Stride
2. Padding

53



Stride

 (7×7)          W (3×3) with stride =1          C  (5×5)  

 (7×7)          W (3×3) with stride =2          C  (3×3)  

54 → Worksheet #5 (“CNN Activation Map”)



Padding
9 0 0 1
1 0 1 0
0 1 1 2
2 1 0 1

0 0 0
0 1 0
0 0 0

0 1
1 1

filter should pick up high values surrounded by low values 

9 not picked up ;-( 

0 0 0 0 0 0
0 9 0 0 1 0
0 1 0 1 0 0
0 0 1 1 2 0
0 2 1 0 1 0
0 0 0 0 0 0

0 0 0
0 1 0
0 0 0

9 0 0 1
1 0 1 0
0 1 1 2
2 1 0 1

9 picked up ;-) 

55



Learn Several Filters

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

 So we create 1 activation map per filter

5x5

56



Pooling Layer

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

 Used to:
 To reduce the size of the activation maps
 So that we reduce the number of parameters of the 

network and hence avoid overfitting.
 Several strategies:

 Max pooling

 Average pooling
 …

429 505 686 856
261 792 412 640
633 653 851 751
608 913 713 657

792 856
913 851

429 505 686 856
261 792 412 640
633 653 851 751
608 913 713 657

496.8 648.5
701.8 743

57

 → Worksheet #5 (“Pooling Layer”)



Architecture of a CNN

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

 Stack:
 Convolutional Layers
 Pooling Layers

 Finish off with:
 A fully connected layer at the end for the final classification

58



Learning a Hierarchy of Features

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/59



Example of a CNN

http://cs231n.github.io/convolutional-networks/ 60



Successful CNN Networks

http://cs231n.github.io/convolutional-networks/

 LeNet
 First successful applications of CNNs 
 Developed by Yann LeCun in the 1990’s
 used to read zip codes, digits, etc.

 AlexNet
 First work that popularized CNNs for 

computer vision
 developed by Alex Krizhevsky, Ilya 

Sutskever and Geoff Hinton (U. of 
Toronto)

 In 2012 significantly outperformed all 
teams at the ImageNet ILSVRC challenge

61

http://yann.lecun.com/exdb/lenet/
http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf


Why now?
1. Basic science

 Backpropagation did not work / overfitting… 
 now: developed method for training, better activation functions, 

better architectures…. 
 Need for lots training data… 
 now: we have massive amounts + unsupervised pre-training

2. GPU computing
 Neural networks take very very long to train… (days, weeks) 
 now: use of GPU’s which are optimized for very fast matrix 

multiplication
3. Open Access to resources

 now : Access to DL methods, code and frameworks
 now : Fast turnaround from idea to implementation

62



History of AI

https://
www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111
http://www.cormix.info/images/RuleTreeExample.jpg

Rules written by experts 
(eg. linguistics, medical doctors,…)

63

https://www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111
https://www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111


History of AI Rules learns via the data ;-)
But: features identified by the experts 
(eg. linguistics, medical doctors,…) 

64



History of AI
Rules AND 
features 
learned from the data

65



Conclusion
 Deep Learning is thriving !

 vision
 image processing
 speech recognition
 natural language processing
 …

 Canada is a world leader in Deep Learning
1. Montreal: (Bengio et al.)   MILA
2. Toronto: (Hinton et al.)  Vector Institute
3. Edmonton: (Sutton et al.) AMII 

66

https://mila.quebec/en/
http://vectorinstitute.ai/
https://www.amii.ca/