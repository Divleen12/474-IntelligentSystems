COMP 472 Artificial Intelligence (Winter 2024)

Worksheet #4: Artificial Neural Networks

Perceptron. Calculate your first neuron activation for the Perceptron (only 100 billion−1 more to go!):

Activation function:

f(~x) =

{
1, if ~x · ~w ≥ threshold

0, otherwise

(use a threshold of 0.55):
f(~x) =

Perceptron Learning. Ok, so for the first training example, the perceptron did not produce the right
output. To learn the correct result, it has to adjust the weights: w′ = w + ∆w, with ∆w = η(T − O),
where we set η = 0.05 (our learning rate). The threshold stays at 0.55. T is the expected output and O the
output produced by the perceptron (= f(~x)).

Student w1 w2 w3 w4 f(~x) ok? ∆w

Richard 0.2 0.2 0.2 0.2

Alan

Alison

(1) Start by computing the output f(~x) for Richard
as before.
(2) Check if the computed output O is correct or not
by comparing it with the expected output T .
(3) If the output was not correct, compute ∆w.
(4) Write down the new weights in the next row (Alan).
Remember to only update weights where the current
sample (Richard) had an active connection (i.e., with
non-zero input, here “No” = 0, “Yes”= 1).
(5) Now repeat the steps, computing the output for
Alan using the updated weights.

Delta Rule. In the generalized delta rule for training the perceptron, we add a bias input that is always
one and has its own weight (here w3). Weight changes ∆wi now take the input value xi into account. We
want the perceptron to learn the two-dimensional data shown on the right:

Assume we use the sign function and set the learning rate η = 0.2. The weights are initialized randomly as
shown in the table. Apply the generalized delta rule for updating the weights:

sign(x) =

{
1 if x ≥ 0

−1 otherwise

∆wi = η(T −O)xi

w′ = w + ∆w

Data w1 w2 w3 f(~x) ok? ∆w1 ∆w2 ∆w3

#1 0.75 0.5 -0.6

#2

#3

#4

COMP 472 Artificial Intelligence (Winter 2024)

Worksheet #4: Artificial Neural Networks

Perceptron. Calculate your first neuron activation for the Perceptron (only 100 billion—1 more to go!):

First last year? 1 Activation function:

0.2
Male? 1 oN First this year? - 1, if #-wW > threshold

02 0, otherwise
9
Works hard? 0 02
aks al? (use a threshold of 0.55):
Dri ? 1 ~
sa lot f(Z) _

Perceptron Learning. Ok, so for the first training example, the perceptron did not produce the right
output. To learn the correct result, it has to adjust the weights: w’ = w+ Aw, with Aw = n(T — O),
where we set 7 = 0.05 (our learning rate). The threshold stays at 0.55. T is the expected output and O the
output produced by the perceptron (= f(Z)).

Features (x,) Output (1) Start by computing the output f(z) for Richard
Student | ‘A’ last year? | Male? | Works hard? | Drinks? | ‘A’ this year? as before.
Richard | Yes Yes. |No Yes No (2) Check if the computed output O is correct or not
Alan Yes Yes. | Yes No Yes by comparing it with the expected output T.
Alison | No No | Yes No No (3) If the output was not correct, compute Aw.
(4) Write down the new weights in the next row (Alan).
= Remember to only update weights where the current
Student vy we ws ws J) ok? Aw sample (Richard) had an active connection (i.e., with
Richard 0.2 0.2 0.2 0.2 non-zero input, here “No” = 0, “Yes”= 1).

Alan

(5) Now repeat the steps, computing the output for

Alison

Alan using the updated weights.

Delta Rule. In the generalized delta rule for training the perceptron, we add a bias input that is always
one and has its own weight (here w3). Weight changes Aw; now take the input value x; into account. We
want the perceptron to learn the two-dimensional data shown on the right:

X; xa Output
x, 1.0 1.0 1
We Goa) 9.4 6.4 -|
2 —<—<$}
2.5 2.1 l
bias
8.0 7.7 -1
a4 OD) 1

Assume we use the sign function and set the learning rate 7 = 0.2. The weights are initialized randomly as
shown in the table. Apply the generalized delta rule for updating the weights:

Data Wy we W3 f(z) ok? Aw, Awe Aw;
. 1 ifx>0
sign(xz) = ; #1 0.75 0.5 -0.6
—1 otherwise
#2
Aw; = n(T — O)x; 43
w =wt+Aw HA





COMP 472 Worksheet: Artificial Neural Networks Winter 2024

Neural Network for XOR: Backpropagation. To learn a non-linearly separable function like XOR, we’ll
use a neural network with a hidden layer. The weights have been initialized randomly (note: here, the bias
is set to −1):

x1 x2 x1 XOR x2

1 1 0

0 0 0

1 0 1

0 1 1

Oi = sigmoid

∑
j

wjixj


=

1

1 + e−(
∑

j wjixj)

Step 1. Compute the output for the three neurons O3, O4 and O5 for the first input (x1 = 1, x2 = 1):
O3 = O4 = O5 =

Step 2. The next step is to calculate the error

δo ← g′(xo)× Erro = Oo(1−Oo)× (Oo − To)

starting from the output neuronO5: δ5 = O5(1−O5)×(O5−T5) =

Step 3. Now we calculate the error terms for the hidden layer:

δh ← g′(xh)× Errh = Oh(1−Oh)×
∑

k∈outputs
δkwhk

For the two neurons (3), (4) in the hidden layer:

• δ3 = O3(1−O3)δ5w35 =

• δ4 = O4(1−O4)δ5w45 =

Step 4. Now we compute our weight changes, using a constant learning rate η = 0.1:

∆wij = −ηδjxi

• ∆w14 =

• ∆w24 =

• ∆w45 =

• ∆Θ5 =

Step 5. And finally, we update the weights (wij ← wij + ∆wij):

• w14 = w14 + ∆w14 =

• w24 = w24 + ∆w24 =

• w45 = w45 + ∆w45 =

• Θ5 = Θ5 + ∆Θ5 =